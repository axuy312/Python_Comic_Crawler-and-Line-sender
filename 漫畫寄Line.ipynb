{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9','Accept-Encoding': 'gzip, deflate, br','Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh-CN;q=0.7,zh;q=0.6','Cache-Control': 'max-age=0','Connection': 'keep-alive','Cookie': 'frombot=1; DM5_MACHINEKEY=667603e3-f455-4ec2-8b45-5d2c1a7ce73f; SERVERID=api03; UM_distinctid=1733820c1f1181-08ea685a98e3d2-4353760-384000-1733820c1f288e; dm5_newsearch=%5b%7b%22Title%22%3a%22%e9%9b%bb%e9%8b%b8%e4%ba%ba%22%2c%22Url%22%3a%22%5c%2fsearch%3ftitle%3d%25E9%259B%25BB%25E9%258B%25B8%25E4%25BA%25BA%26language%3d1%22%7d%2c%7b%22Title%22%3a%22%e7%94%b5%e9%94%af%e4%ba%ba%22%2c%22Url%22%3a%22%5c%2fsearch%3ftitle%3d%25E7%2594%25B5%25E9%2594%25AF%25E4%25BA%25BA%26language%3d1%22%7d%5d; ComicHistoryitem_zh=; CNZZDATA1257110450=222295925-1594371969-https%253A%252F%252Fwww.google.com%252F%7C1594374141','Host': 'www.manhuaren.com','If-Modified-Since': 'Friday, 10 July 2020 11:00:45','If-None-Match': '637300044453577807-0---0-2_20-3','Sec-Fetch-Dest': 'document','Sec-Fetch-Mode': 'navigate','Sec-Fetch-Site': 'none','Sec-Fetch-User': '?1','Upgrade-Insecure-Requests': '1','User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'}\n",
    "\n",
    "headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh-CN;q=0.7,zh;q=0.6',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Cookie': 'frombot=1; DM5_MACHINEKEY=667603e3-f455-4ec2-8b45-5d2c1a7ce73f; SERVERID=api03; UM_distinctid=1733820c1f1181-08ea685a98e3d2-4353760-384000-1733820c1f288e; dm5_newsearch=%5b%7b%22Title%22%3a%22%e9%9b%bb%e9%8b%b8%e4%ba%ba%22%2c%22Url%22%3a%22%5c%2fsearch%3ftitle%3d%25E9%259B%25BB%25E9%258B%25B8%25E4%25BA%25BA%26language%3d1%22%7d%2c%7b%22Title%22%3a%22%e7%94%b5%e9%94%af%e4%ba%ba%22%2c%22Url%22%3a%22%5c%2fsearch%3ftitle%3d%25E7%2594%25B5%25E9%2594%25AF%25E4%25BA%25BA%26language%3d1%22%7d%5d; ComicHistoryitem_zh=; CNZZDATA1257110450=222295925-1594371969-https%253A%252F%252Fwww.google.com%252F%7C1594374141',\n",
    "    'Host': 'www.manhuaren.com',\n",
    "    'If-Modified-Since': 'Friday, 10 July 2020 11:00:45',\n",
    "    'If-None-Match': '637300044453577807-0---0-2_20-3',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh-CN;q=0.7,zh;q=0.6'}\n",
    "\n",
    "urlBase = 'https://www.manhuaren.com'\n",
    "url = 'https://www.manhuaren.com/manhua-haizeiwang-onepiece/?from=/manhua-list/'\n",
    "res = requests.get(url, headers = headers)\n",
    "#res.encoding = 'UTF-8'\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from opencc import OpenCC\n",
    "import lxml\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "liList = soup.findAll(\"a\",{\"class\":\"chapteritem\"})\n",
    "comic_title = OpenCC('s2twp').convert(soup.findAll(\"p\",{\"class\":\"detail-main-info-title\"})[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#liList\n",
    "comic_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opencc import OpenCC\n",
    "comics = []\n",
    "links = []\n",
    "for li in liList:\n",
    "    #檢體轉繁體\n",
    "    title_ch = li.text\n",
    "    title_tw = OpenCC('s2twp').convert(title_ch)\n",
    "    comics.append([title_tw, title_ch, urlBase+li.get('href')])\n",
    "    #links.append(urlBase+li.get('href'))\n",
    "comics.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "comic_df = pandas.DataFrame(comics, columns = ['series-tw', 'series-cn', 'link'])\n",
    "#print(comic_df)\n",
    "#comic_df.columns = ['series', 'link']\n",
    "comic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "with lite.connect('./sqlite/'+comic_title+'.sqlite') as db:\n",
    "    comic_df.to_sql('comics', con = db, if_exists='replace')\n",
    "comic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with lite.connect('./sqlite/'+comic_title+'.sqlite') as db:\n",
    "    df2 = pandas.read_sql_query('select \"series-tw\",\"series-cn\" from comics', con = db)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from opencc import OpenCC\n",
    "import sqlite3 as lite\n",
    "import numpy as np\n",
    "import pandas, requests\n",
    "\n",
    "\n",
    "\n",
    "def check_comics():\n",
    "    # 檢查資料庫最新的漫畫集數\n",
    "    with lite.connect('./sqlite/comics.sqlite') as db:\n",
    "        dbCheck = pandas.read_sql_query('select series from comics', con = db)\n",
    "\n",
    "    datas = []\n",
    "    with lite.connect('./sqlite/comics.sqlite') as db:\n",
    "        datas = pandas.read_sql_query('select series, link from comics', con = db)\n",
    "  \n",
    "    print(datas)\n",
    "    \n",
    "    \n",
    "    # 利用網路爬蟲檢查網路上最新的集數\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    new_comics = []\n",
    "    for rec in soup.findAll('a', {'class', 'chapteritem'}):\n",
    "        title = rec.text\n",
    "        #title = OpenCC('s2twp').convert(rec.text)\n",
    "        if not (title in dbCheck.values):\n",
    "            new_comics.append([title,urlBase+rec.get('href')])\n",
    "            print(\"新增\"+title)\n",
    "    new_comics.reverse()\n",
    "     \n",
    "    if not(new_comics == []):\n",
    "        comics = np.append(datas.values,np.array(new_comics),0)\n",
    "        comic_df = pandas.DataFrame(comics, columns = ['series', 'link'])\n",
    "        with lite.connect('./sqlite/comics.sqlite') as db:\n",
    "            comic_df.to_sql('comics', con = db, if_exists='replace')\n",
    "    #print(new_comics)\n",
    "    \n",
    "    return new_comics\n",
    "\n",
    "#test\n",
    "\"\"\"\n",
    "new_data = check_comics()\n",
    "if new_data == []:\n",
    "    print('沒有更新')\n",
    "else:\n",
    "    print(np.array(new_data))\n",
    "\n",
    "with lite.connect('./sqlite/comics.sqlite') as db:\n",
    "    alldata = pandas.read_sql_query('select * from comics', con = db)\n",
    "print(\"所有資料\\n\")\n",
    "print(alldata)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import requests, re, time, os, shutil\n",
    "\n",
    "\n",
    "def getImgLink(link):\n",
    "    driver = webdriver.Chrome('C:\\\\Users\\\\axuy312\\\\Anaconda_BigData\\\\高階驗證碼\\\\driver\\\\chromedriver.exe')\n",
    "    driver.get(link)\n",
    "    imgLinks = []\n",
    "    eles = driver.find_elements_by_class_name('lazy')\n",
    "    for e in eles:\n",
    "        imgLinks.append(e.get_attribute('src'))\n",
    "    #print(imgLink)\n",
    "    driver.close()\n",
    "    print()\n",
    "    return imgLinks\n",
    "\n",
    "def getComic(link, series):\n",
    "    res = requests.get(link, headers=headers)\n",
    "    #print(res.text)\n",
    "    \n",
    "    print('更新中....')\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    scriptList = soup.findAll(\"script\")\n",
    "    \n",
    "    cnt = 0\n",
    "    MaxCnt = 0\n",
    "    Max = 0\n",
    "    parseString = ''\n",
    "    for script in scriptList:\n",
    "        string = script.find(text=True, recursive=False)\n",
    "        if not(string == None) and len(string) > Max:\n",
    "            Max = len(string)\n",
    "            MaxCnt = cnt\n",
    "            parseString = string\n",
    "        cnt = cnt + 1\n",
    "    initLinks = getImgLink(link)\n",
    "    \n",
    "    page_code = []\n",
    "    for iLink in initLinks:\n",
    "        s = re.findall(r\"[0-9]*_[0-9]*\", iLink)\n",
    "        tmp = s[0].split('_')\n",
    "        page_code.append((int(tmp[0]), int(tmp[1])))\n",
    "    \n",
    "    \n",
    "    print(parseString+\"\\n\")\n",
    "    #string1 = re.findall(\"m=.*\", parseString)\n",
    "    #print(string1)\n",
    "    #print(\"\\n\")\n",
    "    #if string1:\n",
    "    string1 = re.findall(\"[0-9]*_[0-9]*\", parseString)\n",
    "    print(string1)\n",
    "    for s in string1:\n",
    "        tmp = s.split('_')\n",
    "        page_code.append((int(tmp[0]), int(tmp[1])))\n",
    "    page_code = sorted(page_code)\n",
    "    page_link = []\n",
    "    page_link_tmp = []\n",
    "    for code in page_code:\n",
    "        page_link_tmp.append((str(code[0])+'_'+str(code[1])))\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    if page_link_tmp:\n",
    "        page_link.append(page_link_tmp[0])\n",
    "    for p in page_link_tmp:\n",
    "        if not page_link[index] == p:\n",
    "            page_link.append(p)\n",
    "            index = index + 1\n",
    "    \n",
    "    print(np.array(page_link_tmp))\n",
    "    print(np.array(page_link))\n",
    "    \n",
    "    path = \"./comic/\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    path = path + series+\"/\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('正在下載....')\n",
    "    for code in page_link:\n",
    "        #print(code)\n",
    "        #print(re.sub(r\"[0-9]*_[0-9]*\", code, initLink))\n",
    "        imgUrl = re.sub(r\"[0-9]*_[0-9]*\", code, initLinks[0])\n",
    "        imgRes = requests.get(imgUrl, headers=headers)\n",
    "        \n",
    "        with open(path+code.split('_')[0]+'.jpg', 'wb') as f:\n",
    "            f.write(imgRes.content)\n",
    "        time.sleep(1)\n",
    "    print('完成')\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"import pandas\n",
    "with lite.connect('./sqlite/comics.sqlite') as db:\n",
    "    alldata = pandas.read_sql_query('select * from comics', con = db)\n",
    "\n",
    "print('下載中....')\n",
    "getComic(alldata['link'][80], alldata['series'][80])\n",
    "print('完成')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'plcyM9HCHueeZohpFP7a41'\n",
    "callback_url = 'http://127.0.0.1:5000/callback'\n",
    "URL = 'https://notify-bot.line.me/oauth/authorize?response_type=code&client_id='+client_id+'&redirect_uri='+callback_url+'&scope=notify&state=NO_STATE'\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "code = '4aXmld337OG3rLEEenUNyg'\n",
    "client_secret = 'QfbBtdyleMoDatZlXmSa80XS0d9q2827ykpoba5aedg'\n",
    "headers = {\n",
    "    'Content-type': 'application/x-www-form-urlencoded'\n",
    "}\n",
    "\n",
    "payload = {\n",
    " 'code':code,\n",
    " 'client_id': client_id, \n",
    " 'client_secret': client_secret,\n",
    " 'redirect_uri': callback_url,\n",
    " 'grant_type' : 'authorization_code'   \n",
    "}\n",
    "\n",
    "res = requests.post('https://notify-bot.line.me/oauth/token', data = payload, headers = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = '7dNtNbMCQG4BE35yTc1ILhIabgk1B2ON5oopivArbRq'\n",
    "headers = {\n",
    "    'Authorization': 'Bearer '+token\n",
    "}\n",
    "\n",
    "load = {\n",
    " 'message':'\\n', \n",
    "}\n",
    "\n",
    "files = {\n",
    "    'imageFiles' : open('./comic/第71話/2.jpg', 'rb')\n",
    "}\n",
    "\n",
    "res = requests.post('https://notify-api.line.me/api/notify', data = load, files=files, headers = headers)\n",
    "print(res.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def send_comic_page(fileAry):\n",
    "    token = '7dNtNbMCQG4BE35yTc1ILhIabgk1B2ON5oopivArbRq'\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer '+token\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "     'message':'最新的漫畫', \n",
    "    }\n",
    "    \n",
    "    res = requests.post('https://notify-api.line.me/api/notify', data = payload, headers = headers)\n",
    "    print(res.json())\n",
    "    payload = {\n",
    "     'message':'\\n', \n",
    "    }\n",
    "    for f in fileAry:\n",
    "        files = {\n",
    "            'imageFile': open(f, 'rb')\n",
    "        }\n",
    "        print('正在寄送'+f)\n",
    "        #res = requests.post('https://notify-api.line.me/api/notify', data = payload, files=files, headers = headers)\n",
    "        time.sleep(1)\n",
    "        print(res.json())\n",
    "    #return res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas\n",
    "import sqlite3 as lite\n",
    "def send_comics(s):\n",
    "    exist = False\n",
    "    with lite.connect('./sqlite/comics.sqlite') as db:\n",
    "        allData = pandas.read_sql_query('select * from comics', con = db)\n",
    "        exist = s in allData.values\n",
    "    if(exist):\n",
    "        path = './comic/'+s+'/'\n",
    "        if not os.path.isdir(path):\n",
    "            with lite.connect('./sqlite/comics.sqlite') as db:\n",
    "                sel = pandas.read_sql_query('select series, link from comics where series = \"'+s+'\"', con = db)\n",
    "            getComic(sel['link'][0], sel['series'][0])\n",
    "        ary = []\n",
    "        for f in os.listdir(path):\n",
    "            ary.append(int(f.replace('.jpg', '')))\n",
    "        ary.sort()\n",
    "        fileAry = []\n",
    "        print(ary)\n",
    "        for c in ary:\n",
    "            #print('{}/{}.jpg'.format(path,c))\n",
    "            fileAry.append(path+str(c)+'.jpg')\n",
    "        send_comic_page(fileAry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_comics('第71話')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最終程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新中....\n",
      "['1_8604' '2_7724' '3_2468' '4_1722' '5_4759' '6_3775' '7_8470' '8_4521'\n",
      " '9_7534' '10_4867' '11_3668' '12_5095' '13_5035' '14_5674' '15_4556'\n",
      " '16_4629']\n",
      "正在下載....\n",
      "檔案格式更換 (png -> jpg)\n",
      "檔案格式更換 (png -> jpg)\n",
      "檔案格式更換 (png -> jpg)\n",
      "完成\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "{'message': 'ok', 'status': 200}\n",
      "正在寄送./Comics/海賊王/第853話/1.jpg\n",
      "Token更換: BnR8Jkwot7WmXYH3AFaxHEMKKyqSXybYU292L66En8a -> MjJokRIA4Bkg5ql09hKh4pM20Nvhq9B9EDmCoGgdDaN\n",
      "Token更換: MjJokRIA4Bkg5ql09hKh4pM20Nvhq9B9EDmCoGgdDaN -> A5hxqTSfp6XkMT1Rrrf44WaCUQL9A92Fdg6eOQ8ADqZ\n",
      "正在寄送./Comics/海賊王/第853話/2.jpg\n",
      "正在寄送./Comics/海賊王/第853話/3.jpg\n",
      "正在寄送./Comics/海賊王/第853話/4.jpg\n",
      "正在寄送./Comics/海賊王/第853話/5.jpg\n",
      "正在寄送./Comics/海賊王/第853話/6.jpg\n",
      "正在寄送./Comics/海賊王/第853話/7.jpg\n",
      "正在寄送./Comics/海賊王/第853話/8.jpg\n",
      "正在寄送./Comics/海賊王/第853話/9.jpg\n",
      "正在寄送./Comics/海賊王/第853話/10.jpg\n",
      "正在寄送./Comics/海賊王/第853話/11.jpg\n",
      "正在寄送./Comics/海賊王/第853話/12.jpg\n",
      "正在寄送./Comics/海賊王/第853話/13.jpg\n",
      "正在寄送./Comics/海賊王/第853話/14.jpg\n",
      "正在寄送./Comics/海賊王/第853話/15.jpg\n",
      "正在寄送./Comics/海賊王/第853話/16.jpg\n",
      "更新中....\n",
      "['1_3400' '2_9201' '3_9744' '4_1610' '5_5508' '6_8839' '7_9052' '8_6795'\n",
      " '9_1590' '10_8513' '11_9039' '12_7335' '13_2417' '14_9658' '15_7641'\n",
      " '16_5769' '17_6709']\n",
      "正在下載....\n",
      "完成\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "{'message': 'ok', 'status': 200}\n",
      "正在寄送./Comics/海賊王/第854話/1.jpg\n",
      "正在寄送./Comics/海賊王/第854話/2.jpg\n",
      "正在寄送./Comics/海賊王/第854話/3.jpg\n",
      "正在寄送./Comics/海賊王/第854話/4.jpg\n",
      "正在寄送./Comics/海賊王/第854話/5.jpg\n",
      "正在寄送./Comics/海賊王/第854話/6.jpg\n",
      "正在寄送./Comics/海賊王/第854話/7.jpg\n",
      "正在寄送./Comics/海賊王/第854話/8.jpg\n",
      "正在寄送./Comics/海賊王/第854話/9.jpg\n",
      "正在寄送./Comics/海賊王/第854話/10.jpg\n",
      "正在寄送./Comics/海賊王/第854話/11.jpg\n",
      "正在寄送./Comics/海賊王/第854話/12.jpg\n",
      "Token更換: A5hxqTSfp6XkMT1Rrrf44WaCUQL9A92Fdg6eOQ8ADqZ -> d5sCkxII1Dju9FE0eAbWdjsILGdeneiQMq9ARanj2u8\n",
      "正在寄送./Comics/海賊王/第854話/13.jpg\n",
      "正在寄送./Comics/海賊王/第854話/14.jpg\n",
      "正在寄送./Comics/海賊王/第854話/15.jpg\n",
      "正在寄送./Comics/海賊王/第854話/16.jpg\n",
      "正在寄送./Comics/海賊王/第854話/17.jpg\n",
      "更新中....\n",
      "['1_3847' '2_5442' '3_2521' '4_7068' '5_6905' '6_4914' '7_5509' '8_9556'\n",
      " '9_1054' '10_5845' '11_1922' '12_8269' '13_5705' '14_7420' '15_7436'\n",
      " '16_5795']\n",
      "正在下載....\n",
      "檔案格式更換 (png -> jpg)\n",
      "完成\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "{'message': 'ok', 'status': 200}\n",
      "正在寄送./Comics/海賊王/第855話/1.jpg\n",
      "正在寄送./Comics/海賊王/第855話/2.jpg\n",
      "正在寄送./Comics/海賊王/第855話/3.jpg\n",
      "正在寄送./Comics/海賊王/第855話/4.jpg\n",
      "正在寄送./Comics/海賊王/第855話/5.jpg\n",
      "正在寄送./Comics/海賊王/第855話/6.jpg\n",
      "正在寄送./Comics/海賊王/第855話/7.jpg\n",
      "正在寄送./Comics/海賊王/第855話/8.jpg\n",
      "正在寄送./Comics/海賊王/第855話/9.jpg\n",
      "正在寄送./Comics/海賊王/第855話/10.jpg\n",
      "正在寄送./Comics/海賊王/第855話/11.jpg\n",
      "正在寄送./Comics/海賊王/第855話/12.jpg\n",
      "正在寄送./Comics/海賊王/第855話/13.jpg\n",
      "正在寄送./Comics/海賊王/第855話/14.jpg\n",
      "正在寄送./Comics/海賊王/第855話/15.jpg\n",
      "正在寄送./Comics/海賊王/第855話/16.jpg\n",
      "更新中....\n",
      "['1_2432' '2_5092' '3_6542' '4_4536' '5_6533' '6_2250' '7_2657' '8_3380'\n",
      " '9_5714' '10_1790' '11_1877' '12_6988' '13_8212' '14_2103' '15_5324'\n",
      " '16_5603' '17_3087']\n",
      "正在下載....\n",
      "完成\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "{'message': 'ok', 'status': 200}\n",
      "正在寄送./Comics/海賊王/第856話/1.jpg\n",
      "正在寄送./Comics/海賊王/第856話/2.jpg\n",
      "正在寄送./Comics/海賊王/第856話/3.jpg\n",
      "正在寄送./Comics/海賊王/第856話/4.jpg\n",
      "正在寄送./Comics/海賊王/第856話/5.jpg\n",
      "正在寄送./Comics/海賊王/第856話/6.jpg\n",
      "正在寄送./Comics/海賊王/第856話/7.jpg\n",
      "正在寄送./Comics/海賊王/第856話/8.jpg\n",
      "正在寄送./Comics/海賊王/第856話/9.jpg\n",
      "正在寄送./Comics/海賊王/第856話/10.jpg\n",
      "正在寄送./Comics/海賊王/第856話/11.jpg\n",
      "正在寄送./Comics/海賊王/第856話/12.jpg\n",
      "正在寄送./Comics/海賊王/第856話/13.jpg\n",
      "正在寄送./Comics/海賊王/第856話/14.jpg\n",
      "正在寄送./Comics/海賊王/第856話/15.jpg\n",
      "正在寄送./Comics/海賊王/第856話/16.jpg\n",
      "正在寄送./Comics/海賊王/第856話/17.jpg\n",
      "更新中....\n",
      "['1_6021' '2_3061' '3_4388' '4_7823' '5_8002' '6_6297' '7_3152' '8_9441'\n",
      " '9_1141' '10_5664' '11_6761' '12_3288' '13_2063' '14_3760' '15_7560'\n",
      " '16_8644']\n",
      "正在下載....\n",
      "檔案格式更換 (png -> jpg)\n",
      "完成\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "{'message': 'ok', 'status': 200}\n",
      "正在寄送./Comics/海賊王/第857話/1.jpg\n",
      "正在寄送./Comics/海賊王/第857話/2.jpg\n",
      "正在寄送./Comics/海賊王/第857話/3.jpg\n",
      "正在寄送./Comics/海賊王/第857話/4.jpg\n",
      "正在寄送./Comics/海賊王/第857話/5.jpg\n",
      "正在寄送./Comics/海賊王/第857話/6.jpg\n",
      "正在寄送./Comics/海賊王/第857話/7.jpg\n",
      "正在寄送./Comics/海賊王/第857話/8.jpg\n",
      "正在寄送./Comics/海賊王/第857話/9.jpg\n",
      "正在寄送./Comics/海賊王/第857話/10.jpg\n",
      "正在寄送./Comics/海賊王/第857話/11.jpg\n",
      "正在寄送./Comics/海賊王/第857話/12.jpg\n",
      "Token更換: d5sCkxII1Dju9FE0eAbWdjsILGdeneiQMq9ARanj2u8 -> QfU7JVwqTLqx1gnpwUeSrjGn0ARNSvtsdXLUjoMGOhq\n",
      "正在寄送./Comics/海賊王/第857話/13.jpg\n",
      "正在寄送./Comics/海賊王/第857話/14.jpg\n",
      "正在寄送./Comics/海賊王/第857話/15.jpg\n",
      "正在寄送./Comics/海賊王/第857話/16.jpg\n",
      "更新中....\n",
      "['1_3089' '2_1090' '3_2537' '4_2672' '5_8884' '6_2963' '7_5988' '8_7413'\n",
      " '9_6908' '10_9402' '11_6575' '12_6622' '13_6610' '14_6730' '15_9562'\n",
      " '16_2744' '17_1667']\n",
      "正在下載....\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "檔案格式更換 (jpg -> png)\n",
      "完成\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "{'message': 'ok', 'status': 200}\n",
      "正在寄送./Comics/海賊王/第858話/1.jpg\n",
      "正在寄送./Comics/海賊王/第858話/2.jpg\n",
      "正在寄送./Comics/海賊王/第858話/3.jpg\n",
      "正在寄送./Comics/海賊王/第858話/4.jpg\n",
      "正在寄送./Comics/海賊王/第858話/5.jpg\n",
      "正在寄送./Comics/海賊王/第858話/6.jpg\n",
      "正在寄送./Comics/海賊王/第858話/7.jpg\n",
      "正在寄送./Comics/海賊王/第858話/8.jpg\n",
      "正在寄送./Comics/海賊王/第858話/9.jpg\n",
      "正在寄送./Comics/海賊王/第858話/10.jpg\n",
      "正在寄送./Comics/海賊王/第858話/11.jpg\n",
      "正在寄送./Comics/海賊王/第858話/12.jpg\n",
      "正在寄送./Comics/海賊王/第858話/13.jpg\n",
      "正在寄送./Comics/海賊王/第858話/14.jpg\n",
      "正在寄送./Comics/海賊王/第858話/15.jpg\n",
      "正在寄送./Comics/海賊王/第858話/16.jpg\n",
      "正在寄送./Comics/海賊王/第858話/17.jpg\n",
      "更新中....\n",
      "['2_3580' '3_9640' '4_8769' '5_6659' '6_5197' '7_5667' '8_9298' '9_5655'\n",
      " '10_6204' '11_3779' '12_1620' '13_2503' '14_4342' '15_7532' '16_4892'\n",
      " '17_5373']\n",
      "正在下載....\n",
      "完成\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "{'message': 'ok', 'status': 200}\n",
      "正在寄送./Comics/海賊王/第859話/2.jpg\n",
      "正在寄送./Comics/海賊王/第859話/3.jpg\n",
      "正在寄送./Comics/海賊王/第859話/4.jpg\n",
      "正在寄送./Comics/海賊王/第859話/5.jpg\n",
      "正在寄送./Comics/海賊王/第859話/6.jpg\n",
      "正在寄送./Comics/海賊王/第859話/7.jpg\n",
      "正在寄送./Comics/海賊王/第859話/8.jpg\n",
      "正在寄送./Comics/海賊王/第859話/9.jpg\n",
      "正在寄送./Comics/海賊王/第859話/10.jpg\n",
      "正在寄送./Comics/海賊王/第859話/11.jpg\n",
      "正在寄送./Comics/海賊王/第859話/12.jpg\n",
      "正在寄送./Comics/海賊王/第859話/13.jpg\n",
      "正在寄送./Comics/海賊王/第859話/14.jpg\n",
      "正在寄送./Comics/海賊王/第859話/15.jpg\n",
      "正在寄送./Comics/海賊王/第859話/16.jpg\n",
      "正在寄送./Comics/海賊王/第859話/17.jpg\n",
      "結束\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from opencc import OpenCC\n",
    "import numpy as np\n",
    "import sqlite3 as lite\n",
    "import requests, re, time, os, shutil, pandas, time\n",
    "\n",
    "selSeries = ''\n",
    "token_index = 0\n",
    "tokens = [\n",
    " 'BnR8Jkwot7WmXYH3AFaxHEMKKyqSXybYU292L66En8a',\n",
    " 'MjJokRIA4Bkg5ql09hKh4pM20Nvhq9B9EDmCoGgdDaN',\n",
    " 'A5hxqTSfp6XkMT1Rrrf44WaCUQL9A92Fdg6eOQ8ADqZ',\n",
    " 'd5sCkxII1Dju9FE0eAbWdjsILGdeneiQMq9ARanj2u8',\n",
    " 'QfU7JVwqTLqx1gnpwUeSrjGn0ARNSvtsdXLUjoMGOhq',\n",
    " 'XtcLhlVhRH7kR2Yo0QFfL78Dy31h8xseLEoN4hOm5UY',\n",
    " 'zQCI84AEJcRzQVSin8PT6jB43YFGwHn6fDI48l79FB1',\n",
    " '48eL4RkIDOjebvFG6KTEQQYTCGn37I8YyKVbn7LyK7f',\n",
    " 'K4iD5rM44z6FMmS8cNEhCd8jtC5haXvwRNLtmqMm5sc',\n",
    " 'WXM6BjIpvFuSus2YaXdnRANiTqnIx4Ez4mKTrD284CZ',\n",
    " 'icm4Vqf5wKBRDWoQH0MBGH9sJIABp1WmirEUGKhD7sW',\n",
    " 'Now3hPicu6O9HY3d2j4gbAq5GQ4goSoriA8nZJKkt5m',\n",
    " 'B8B9sxeuzjFIFqnpUDraAGFHaitKBrUf54raCgbu5Vg',\n",
    " 'XlRs1pRSpDtVjF1pFvMw6apj3hxZxlZwKQ1Se1BiI1X',\n",
    " 'YtN2gXo3yhAUZkovFIPhbKZIUI8VbSyCUD7LpKK2VLS',\n",
    " 'fIoEjkm2GWNf6G30I8SebDUOEwv8FeFQVyOVderpN8p',\n",
    " 'yVoZie372AhSqb9CebkKAqeMFML8HcnaYhsyxiC2gew',\n",
    " 'mQQsIWIt88JPvzdUrKBG6R4mlbRdDphFu28Fo4UuCfN',\n",
    " '8yC2gJnvpIEa6ltRg4TiHgkefaT4KOYj9jSTA3ZqEwO',\n",
    " 'nslOIiEIxOY59ucDAx69e4DSCIs6icsr5Un0HUK0Ekt',\n",
    " 'rXTKa94IJx2VI7As4yo8p2XPpHSjA1XDX6ElbD77fNB',\n",
    " 'uLtfA4CzmH15FT3ha4S9c8h97jqiSFClYPxQGyPvWIi',\n",
    " 'r3msthbJ0Cbkbp4uCI6mPWOsegZa8oMY7Ith0PN87u7',\n",
    " 'wksrTHL0EOQQLsZ6qJ1bBFcacjhKgJRZhtLFZTsojUS',\n",
    " 'su5fplrRz53YJfM6hNC8rvibTm1FewWsWVKfDtcrccM'\n",
    "]\n",
    "\n",
    "tokens_size = len(tokens)\n",
    "#print(\"size: \"+str(tokens_size))\n",
    "\n",
    "# Init\n",
    "headers = {'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh-CN;q=0.7,zh;q=0.6'}\n",
    "\n",
    "urlBase = 'https://www.manhuaren.com'\n",
    "url = 'https://www.manhuaren.com/manhua-haizeiwang-onepiece/?from=/manhua-list/'\n",
    "res = requests.get(url, headers = headers)\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "liList = soup.findAll(\"a\",{\"class\":\"chapteritem\"})\n",
    "comic_title = OpenCC('s2twp').convert(soup.findAll(\"p\",{\"class\":\"detail-main-info-title\"})[0].text)\n",
    "\n",
    "#-------------------\n",
    "\n",
    "def check_comics():\n",
    "    # 檢查資料庫最新的漫畫集數\n",
    "    global comic_title\n",
    "    global url\n",
    "    global urlBase\n",
    "    with lite.connect('./sqlite/'+comic_title+'.sqlite') as db:\n",
    "        dbCheck = pandas.read_sql_query('select \"series-tw\", \"series-cn\" from comics', con = db)\n",
    "\n",
    "    datas = []\n",
    "    with lite.connect('./sqlite/'+comic_title+'.sqlite') as db:\n",
    "        datas = pandas.read_sql_query('select \"series-tw\", \"series-cn\", link from comics', con = db)\n",
    "  \n",
    "    #print(datas)\n",
    "    \n",
    "    \n",
    "    # 利用網路爬蟲檢查網路上最新的集數\n",
    "    headers = {'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh-CN;q=0.7,zh;q=0.6'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    new_comics = []\n",
    "    comic_title = OpenCC('s2twp').convert(soup.findAll(\"p\",{\"class\":\"detail-main-info-title\"})[0].text)\n",
    "    for rec in soup.findAll('a', {'class', 'chapteritem'}):\n",
    "        title_ch = rec.text\n",
    "        title_tw = OpenCC('s2twp').convert(title_ch)\n",
    "        if not (title_tw in dbCheck.values):\n",
    "            new_comics.append([title_tw, title_ch,urlBase+rec.get('href')])\n",
    "            print(\"新增\"+title_tw)\n",
    "    new_comics.reverse()\n",
    "     \n",
    "    if not(new_comics == []):\n",
    "        comics = np.append(datas.values,np.array(new_comics),0)\n",
    "        comic_df = pandas.DataFrame(comics, columns = ['series-tw', 'series-ch', 'link'])\n",
    "        with lite.connect('./sqlite/'+comic_title+'.sqlite') as db:\n",
    "            comic_df.to_sql('comics', con = db, if_exists='replace')\n",
    "    #print(new_comics)\n",
    "    \n",
    "    return new_comics\n",
    "\n",
    "def getImgLink(link):\n",
    "    driver = webdriver.Chrome('C:\\\\Users\\\\axuy312\\\\Anaconda_BigData\\\\高階驗證碼\\\\driver\\\\chromedriver.exe')\n",
    "    driver.get(link)\n",
    "    imgLinks = []\n",
    "    eles = driver.find_elements_by_class_name('lazy')\n",
    "    for e in eles:\n",
    "        imgLinks.append(e.get_attribute('src'))\n",
    "    #print(imgLink)\n",
    "    driver.close()\n",
    "    return imgLinks\n",
    "\n",
    "def getComic(link, series):\n",
    "    global comic_title\n",
    "    headers = {'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh-CN;q=0.7,zh;q=0.6'}\n",
    "    res = requests.get(link, headers=headers)\n",
    "    #print(res.text)\n",
    "    \n",
    "    print('更新中....')\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    scriptList = soup.findAll(\"script\")\n",
    "    \n",
    "    cnt = 0\n",
    "    MaxCnt = 0\n",
    "    Max = 0\n",
    "    parseString = ''\n",
    "    for script in scriptList:\n",
    "        string = script.find(text=True, recursive=False)\n",
    "        if not(string == None) and len(string) > Max:\n",
    "            Max = len(string)\n",
    "            MaxCnt = cnt\n",
    "            parseString = string\n",
    "        cnt = cnt + 1\n",
    "    initLinks = getImgLink(link)\n",
    "    \n",
    "    page_code = []\n",
    "    for iLink in initLinks:\n",
    "        s = re.findall(r\"[0-9]*_[0-9]*\", iLink)\n",
    "        tmp = s[0].split('_')\n",
    "        page_code.append((int(tmp[0]), int(tmp[1])))\n",
    "    \n",
    "    \n",
    "    #print(parseString+\"\\n\")\n",
    "    #string1 = re.findall(\"m=.*\", parseString)\n",
    "    #print(string1)\n",
    "    #print(\"\\n\")\n",
    "    #if string1:\n",
    "    string1 = re.findall(\"[0-9]*_[0-9]*\", parseString)\n",
    "    #print(string1)\n",
    "    for s in string1:\n",
    "        tmp = s.split('_')\n",
    "        page_code.append((int(tmp[0]), int(tmp[1])))\n",
    "    page_code = sorted(page_code)\n",
    "    page_link = []\n",
    "    page_link_tmp = []\n",
    "    for code in page_code:\n",
    "        page_link_tmp.append((str(code[0])+'_'+str(code[1])))\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    if page_link_tmp:\n",
    "        page_link.append(page_link_tmp[0])\n",
    "    for p in page_link_tmp:\n",
    "        if not page_link[index] == p:\n",
    "            page_link.append(p)\n",
    "            index = index + 1\n",
    "    \n",
    "    #print(np.array(page_link_tmp))\n",
    "    print(np.array(page_link))\n",
    "    \n",
    "    path = \"./Comics\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    path = path + \"/\" + comic_title\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    path = path + \"/\" + series + \"/\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('正在下載....')\n",
    "    for code in page_link:\n",
    "        #print(code)\n",
    "        #print(re.sub(r\"[0-9]*_[0-9]*\", code, initLink))\n",
    "        imgUrl = re.sub(r\"[0-9]*_[0-9]*\", code, initLinks[0])\n",
    "        imgRes = requests.get(imgUrl, headers=headers)\n",
    "        if imgRes.reason == 'Not Found':\n",
    "            if re.findall(r'\\.jpg', imgUrl):\n",
    "                print(\"檔案格式更換 (jpg -> png)\")\n",
    "                imgUrl = re.sub(r\"\\.jpg\", \".png\", imgUrl)\n",
    "            elif re.findall(r'\\.png', imgUrl):\n",
    "                print(\"檔案格式更換 (png -> jpg)\")\n",
    "                imgUrl = re.sub(r\"\\.png\", \".jpg\", imgUrl)\n",
    "            else:\n",
    "                print('抓不到檔案格式')\n",
    "            imgRes = requests.get(imgUrl, headers=headers)\n",
    "            if not imgRes.reason == 'OK':\n",
    "                print(\"檔案爬不到( At getComic(link, series) \"+comic_title+\"/\"+series+\"/\"+code+\" URL: \"+ imgUrl+\" )\")\n",
    "        with open(path+code.split('_')[0]+'.jpg', 'wb') as f:\n",
    "            f.write(imgRes.content)\n",
    "        time.sleep(1)\n",
    "    print('完成')\n",
    "\n",
    "def send_comic_page(fileAry):\n",
    "    global token_index\n",
    "    global tokens\n",
    "    global tokens_size\n",
    "    global selSeries\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': 'Bearer '+tokens[token_index]\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "     'message':selSeries, \n",
    "    }\n",
    "    \n",
    "    res = requests.post('https://notify-api.line.me/api/notify', data = payload, headers = headers)\n",
    "    print(res.json())\n",
    "    page_cnt=1\n",
    "    for f in fileAry:\n",
    "        files = {\n",
    "            'imageFile': open(f, 'rb')\n",
    "        }\n",
    "        print('正在寄送'+f)\n",
    "        payload = {\n",
    "         'message':'第'+str(page_cnt)+'頁', \n",
    "        }\n",
    "        res = requests.post('https://notify-api.line.me/api/notify', data = payload, files=files, headers = headers)\n",
    "        page_cnt = page_cnt + 1\n",
    "        time.sleep(1)\n",
    "        tmpToken = token_index\n",
    "        while res.json()['message'] == 'Image rate limit exceeded.':\n",
    "            new_tkn = (token_index + 1) % tokens_size\n",
    "            print(\"Token更換: \"+tokens[token_index]+\" -> \"+ tokens[new_tkn])\n",
    "            token_index = new_tkn\n",
    "            if token_index == tmpToken:\n",
    "                print('所有Token皆暫時無法使用!')\n",
    "                return\n",
    "            headers = {\n",
    "                'Authorization': 'Bearer '+tokens[token_index]\n",
    "            }\n",
    "            res = requests.post('https://notify-api.line.me/api/notify', data = payload, files=files, headers = headers)\n",
    "\n",
    "\n",
    "def send_comics(s):\n",
    "    global comic_title\n",
    "    exist = False\n",
    "    with lite.connect('./sqlite/'+comic_title+'.sqlite') as db:\n",
    "        allData = pandas.read_sql_query('select * from comics', con = db)\n",
    "        #print(allData)\n",
    "        exist = s in allData.values\n",
    "    if(exist):\n",
    "        path = './Comics/'+comic_title+'/'+s+'/'\n",
    "        if not os.path.isdir(path):\n",
    "            with lite.connect('./sqlite/'+comic_title+'.sqlite') as db:\n",
    "                sel = pandas.read_sql_query('select \"series-tw\", \"series-cn\",\"link\" from comics where \"series-tw\" = \"'+s+'\" or \"series-cn\" = \"'+s+'\"', con = db)\n",
    "            getComic(sel['link'][0], sel['series-tw'][0])\n",
    "        ary = []\n",
    "        for f in os.listdir(path):\n",
    "            ary.append(int(f.replace('.jpg', '')))\n",
    "        ary.sort()\n",
    "        fileAry = []\n",
    "        print(ary)\n",
    "        for c in ary:\n",
    "            #print('{}/{}.jpg'.format(path,c))\n",
    "            fileAry.append(path+str(c)+'.jpg')\n",
    "        send_comic_page(fileAry)\n",
    "    else:\n",
    "        print(s+\"找不到( At send_comics() )\")\n",
    "    \n",
    "\n",
    "#check_comics()\n",
    "        \n",
    "for i in range(853,860):\n",
    "    selSeries = '第'+str(i)+'話'\n",
    "    send_comics(selSeries)\n",
    "\n",
    "print('結束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh-CN;q=0.7,zh;q=0.6'}\n",
    "\n",
    "urlBase = 'https://www.manhuaren.com'\n",
    "url = 'https://manhua1031-104-250-150-12.cdnmanhua.net/1/432/451925/4_8148.jpg'\n",
    "res = requests.get(url, headers = headers)\n",
    "#res.encoding = 'UTF-8'\n",
    "print(res.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下為增加權杖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'plcyM9HCHueeZohpFP7a41'\n",
    "callback_url = 'http://127.0.0.1:5000/callback'\n",
    "URL = 'https://notify-bot.line.me/oauth/authorize?response_type=code&client_id='+client_id+'&redirect_uri='+callback_url+'&scope=notify&state=NO_STATE'\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "code = ''\n",
    "while True:\n",
    "    code = input(\"Code: \")\n",
    "    if code == '#':\n",
    "        break\n",
    "    client_secret = 'QfbBtdyleMoDatZlXmSa80XS0d9q2827ykpoba5aedg'\n",
    "    headers = {\n",
    "        'Content-type': 'application/x-www-form-urlencoded'\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "     'code':code,\n",
    "     'client_id': client_id, \n",
    "     'client_secret': client_secret,\n",
    "     'redirect_uri': callback_url,\n",
    "     'grant_type' : 'authorization_code'   \n",
    "    }\n",
    "\n",
    "    res = requests.post('https://notify-bot.line.me/oauth/token', data = payload, headers = headers)\n",
    "    tkn = res.json()#['access_token']\n",
    "    print(res.json())\n",
    "    tokens.append(tkn['access_token'])\n",
    "    print(\"Tokens: \")\n",
    "    print(np.array(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtt = 0\n",
    "print(np.array(tokens))\n",
    "for t in tokens:\n",
    "    cmtt = cmtt + 1\n",
    "    print(str(cmtt)+\": \"+t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
